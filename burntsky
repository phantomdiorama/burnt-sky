#!/usr/bin/env python3
"""Burnt Sky"""

import re
import time
import asyncio
import sys
from pathlib import Path
import httpx
from bs4 import BeautifulSoup
import feedparser
import dominate
from dominate.tags import a, footer, h1, h2, hr, link, p
from dominate.util import raw

### check data folder ###
data_path = Path.home() / ".config" / "burntsky"

if data_path.is_dir():
    pass
else:
    data_path.mkdir()
    print("data path created")


### add user feed ###
def addUser(url):
    profile = httpx.get(url, timeout=5)
    soup = BeautifulSoup(profile.text, "html.parser")
    did = soup.find(id="bsky_did")
    handle = soup.find(id="bsky_handle")
    feed = "https://bsky.app/profile/" + did.text + "/rss"
    print("Adding User:")
    print("Handle: " + handle.text)
    print("Feed: " + feed)
    with open(data_path / "users.txt", "a", encoding="utf-8") as users:
        users.write(handle.text + " " + feed + "\n")


if len(sys.argv) == 2:
    addUser(sys.argv[1])
    sys.exit()

### request feeds ###
async def get_async(url):
    async with httpx.AsyncClient() as client:
        return await client.get(url)


urls = []
blusers = []

with open(data_path / "users.txt") as file:
    for line in file:
        short_line = line.split(" ")
        urls.append(short_line[1].strip())


async def launch():
    resps = await asyncio.gather(*map(get_async, urls))
    data = [resp for resp in resps]
    for html in data:
        blusers.append(html)


asyncio.run(launch())


### parse feeds and build out document ###
def buildWebpage():
    print("Running...")
    curr_time = time.strftime("%H:%M", time.localtime())
    last_up = ""
    quote_text = "[contains quote post or other embedded content]"
    blank_char = ""
    doc = dominate.document(title="Burnt Sky")
    with doc.head:
        raw('<meta charset="UTF-8">')
        raw('<meta name="viewport" content="width=device-width, initial-scale=1.0" />')
        link(rel="stylesheet", href="style.css")
        doc += h1("Burnt Sky")
        doc += p("A quiet bluesky reader")
        doc += p("Updated: " + curr_time)
    for i in blusers:
        d = feedparser.parse(i)
        # bail early post has no text
        if "summary" not in d.entries[0]:
            continue
        else:
            rss_title = d.feed.title.split(".")
            file_title = rss_title[0].strip("@ ")
            skeet_link = d.entries[0].link
            # check if post has been seen
            data_file = data_path / file_title
            if Path(data_file).is_file():
                with open(Path(data_file), "r", encoding="utf-8") as dataRead:
                    last_up = dataRead.read()
            if d.entries[0].link == last_up:
                continue
            else:
                # strip quotes and links
                skeet_text = d.entries[0].summary
                regex = re.compile(r"\b(http|www)\S+")
                mo = regex.search(skeet_text)
                if mo is not None:
                    skeet_text = skeet_text.replace(mo.group(), blank_char)
                skeet_text = skeet_text.replace(quote_text, blank_char)
                # add post to document
                doc += h2(rss_title[0])
                doc += p(skeet_text, a("[link]", href=skeet_link))
                with open(Path(data_file), "w", encoding="utf-8") as db:
                    db.write(d.entries[0].link)
    # if no posts
    if "h2" not in doc:
        print("Nobody has posted")
        doc += h2("Nobody has posted")
        doc += p("Or the world has ended")
    # finally add footer
    doc += hr()
    doc += footer("Version 0.1")
    # write file
    with open("burntsky.html", "w", encoding="utf-8") as outputFile:
        outputFile.write(doc.render())


buildWebpage()
